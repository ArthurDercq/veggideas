{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selectivesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import selectivesearch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import selectivesearch\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = '/Users/lieselvranckx/code/ArthurDercq/veggideas/raw_data/test/Carrot/1023.jpg'\n",
    "\n",
    "# Load the image with correct color channels\n",
    "image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw an empty figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the image\n",
    "ax.imshow(image)\n",
    "\n",
    "# Perform selective search to generate candidate regions\n",
    "selective_search_results = selectivesearch.selective_search(image, scale=750, sigma=0.99, min_size=10)\n",
    "min_box_size = 1000\n",
    "\n",
    "# Calculate target aspect ratio range (adjust as per your requirement)\n",
    "target_aspect_ratio = 0.30\n",
    "aspect_ratio_range = 0.35\n",
    "\n",
    "selected_boxes = []\n",
    "for region in selective_search_results[1]:\n",
    "    x, y, w, h = region['rect']\n",
    "    if w * h >= min_box_size:\n",
    "        aspect_ratio = float(w) / h\n",
    "        if abs(aspect_ratio - target_aspect_ratio) <= aspect_ratio_range:\n",
    "            selected_boxes.append((x, y, x + w, y + h))\n",
    "\n",
    "rect = []\n",
    "for i, (x, y, x2, y2) in enumerate(selected_boxes):\n",
    "    rect.append(patches.Rectangle((x, y), x2 - x, y2 - y, edgecolor='green', facecolor='none', linewidth=2))\n",
    "    ax.add_patch(rect[i])\n",
    "\n",
    "    ax.text(x, y, f'Box{i}: {round((aspect_ratio - target_aspect_ratio) * 100, 2)}%', color='red')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = '/Users/lieselvranckx/code/ArthurDercq/veggideas/raw_data/train/brocolis.jpeg'\n",
    "# Load the image with correct color channels\n",
    "image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw an empty figure\n",
    "fig, ax = plt.subplots()\n",
    "# Plot the image\n",
    "ax.imshow(image)\n",
    "\n",
    "selective_search_results = selectivesearch.selective_search(image, scale=750, sigma=0.9, min_size=100)\n",
    "\n",
    "objectness_score_threshold = 0.9\n",
    "minimum_bounding_box_size = 100\n",
    "maximum_aspect_ratio = 3\n",
    "# Calculate target aspect ratio range (adjust as per your requirement)\n",
    "target_aspect_ratio = 0.30\n",
    "aspect_ratio_range = 0.30\n",
    "\n",
    "selected_candidates = []\n",
    "for candidate in selective_search_results[1]:\n",
    "    x, y, w, h = candidate['rect']\n",
    "\n",
    "    # Calculate bounding box size and aspect ratio\n",
    "    bounding_box_size = w * h\n",
    "    aspect_ratio = max(w / h, h / w)\n",
    "\n",
    "    # Check objectness score, bounding box size, and aspect ratio\n",
    "    if (\n",
    "        bounding_box_size >= minimum_bounding_box_size\n",
    "        and aspect_ratio <= maximum_aspect_ratio\n",
    "        and (aspect_ratio - target_aspect_ratio) * 100 > 100\n",
    "    ):\n",
    "        selected_candidates.append(candidate)\n",
    "\n",
    "rect = []\n",
    "for i, candidate in enumerate(selected_candidates):\n",
    "    x, y, w, h = candidate['rect']\n",
    "    rect.append(patches.Rectangle((x, y), w, h, edgecolor='green', facecolor='none', linewidth=2))\n",
    "    ax.add_patch(rect[i])\n",
    "\n",
    "    aspect_ratio = max(w / h, h / w)\n",
    "    ax.text(x, y, f'Box{i}: {round((aspect_ratio - target_aspect_ratio) * 100, 2)}%', color='green')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load latest model from GCS...\n",
      "✅ Latest model downloaded from cloud storage\n"
     ]
    }
   ],
   "source": [
    "from veggideas.registry import load_model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/chou.jpeg'\n",
    "\n",
    "# Load the image\n",
    "#image = cv2.imread(image_path)\n",
    "image = tf.image.decode_image(tf.io.read_file(image_path))\n",
    "\n",
    "# Perform selective search to generate candidate regions\n",
    "selective_search_results = selectivesearch.selective_search(image, scale=750, sigma=0.9, min_size=100)\n",
    "\n",
    "minimum_bounding_box_size = 100\n",
    "maximum_aspect_ratio = 3\n",
    "# Calculate target aspect ratio range (adjust as per your requirement)\n",
    "target_aspect_ratio = 1\n",
    "aspect_ratio_range = 0.30\n",
    "\n",
    "selected_candidates = []\n",
    "for i, candidate in enumerate(selective_search_results[1]):\n",
    "    x, y, w, h = candidate['rect']\n",
    "\n",
    "    # Calculate bounding box size and aspect ratio\n",
    "    bounding_box_size = w * h\n",
    "    aspect_ratio = max(w / h, h / w)\n",
    "\n",
    "    # Exclude the bounding box of the whole picture\n",
    "    image_size = image.shape[0] * image.shape[1]\n",
    "    box_coverage = bounding_box_size / image_size\n",
    "    if box_coverage < 0.98 and bounding_box_size >= minimum_bounding_box_size and aspect_ratio <= maximum_aspect_ratio:\n",
    "        selected_candidates.append({'rect': (x, y, w, h), 'index': i})\n",
    "\n",
    "\n",
    "# Function to calculate the coverage ratio between two bounding boxes\n",
    "def calculate_coverage_ratio(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    intersection_x = max(x1, x2)\n",
    "    intersection_y = max(y1, y2)\n",
    "    intersection_w = min(x1 + w1, x2 + w2) - intersection_x\n",
    "    intersection_h = min(y1 + h1, y2 + h2) - intersection_y\n",
    "\n",
    "    intersection_area = max(0, intersection_w) * max(0, intersection_h)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    return intersection_area / box1_area, box1_area, box2_area\n",
    "\n",
    "# List to store the filtered bounding boxes\n",
    "filtered_boxes = []\n",
    "\n",
    "# Iterate through the selected bounding boxes\n",
    "for i in range(len(selected_candidates)):\n",
    "    current_box = selected_candidates[i]['rect']\n",
    "    is_contained = False\n",
    "\n",
    "    # Check if the coverage ratio of the current box is less than 90% of any other box\n",
    "    for j in range(len(selected_candidates)):\n",
    "        if i != j:\n",
    "            other_box = selected_candidates[j]['rect']\n",
    "            coverage_ratio, current_area, other_area = calculate_coverage_ratio(current_box, other_box)\n",
    "            if coverage_ratio >= 1:\n",
    "                # The current box is contained within another box\n",
    "                is_contained = True\n",
    "                if current_area >= other_area:\n",
    "                    filtered_boxes.append(current_box)\n",
    "                else:\n",
    "                    break\n",
    "    # If the current box is not contained, add it to the filtered list\n",
    "    if not is_contained:\n",
    "        filtered_boxes.append(current_box)\n",
    "\n",
    "\n",
    "unique_tuples = set(filtered_boxes)\n",
    "filtered_list = list(unique_tuples)\n",
    "\n",
    "\n",
    "# # # Create a figure and axes\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the image\n",
    "# ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# rect = []\n",
    "# for i, box in enumerate(filtered_list):\n",
    "#      x, y, w, h = box\n",
    "#      rect.append(patches.Rectangle((x, y), w, h, edgecolor='green', facecolor='none', linewidth=2))\n",
    "#      ax.add_patch(rect[i])\n",
    "\n",
    "#      aspect_ratio = max(w / h, h / w)\n",
    "#      ax.text(x, y, f'Box{i}: {round((aspect_ratio - target_aspect_ratio) * 100, 2)}%', color='green')\n",
    "\n",
    "# # # Show the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subimages =[]\n",
    "# Itérer sur les coordonnées filtrées\n",
    "for coords in filtered_list:\n",
    "    \n",
    "    x, y, w, h = coords\n",
    "\n",
    "    # Extraire l'image à partir des coordonnées\n",
    "    cropped_image = image[y:y+h, x:x+w]\n",
    "    resized_image = tf.image.resize(cropped_image, (224, 224))\n",
    "    final_image = np.expand_dims(resized_image, axis=0)\n",
    "    list_subimages.append(final_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "vegg_list = ['Bean', 'Broccoli',\n",
    "                'Cabbage', 'Capsicum', 'Carrot', 'Cauliflower', 'Cucumber',\n",
    "                'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']\n",
    "\n",
    "predictions = []\n",
    "for image in list_subimages:\n",
    "    prediction = model.predict(image)\n",
    "\n",
    "    pred_class = np.argmax(prediction, axis=-1)[0]\n",
    "\n",
    "    final_prediction = vegg_list[pred_class].lower()\n",
    "\n",
    "    predictions.append(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage',\n",
       " 'cabbage']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cabbage'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_frequent(list):\n",
    "    counter = 0\n",
    "    num = list[0]\n",
    "     \n",
    "    for i in list:\n",
    "        curr_frequency = list.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    " \n",
    "    return num\n",
    " \n",
    "most_frequent(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_images= [\"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/broccoli.jpg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/brocolis.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/capsicums.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/carrot_couli.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/carrots.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/chou.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/cucumber.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/different_veggies.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/patates.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/pumkins.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/pumpkin.jpeg\",\n",
    "                  \"/Users/arthurdercq/code/ArthurDercq/veggideas/raw_data/testing_images/tomato.png\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/arthurdercq/Desktop/eggplants.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/arthurdercq/Desktop/eggplants.jpeg'\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "# Perform selective search to generate candidate regions\n",
    "selective_search_results = selectivesearch.selective_search(image, scale=750, sigma=0.8, min_size=100)\n",
    "minimum_bounding_box_size = 80\n",
    "maximum_aspect_ratio = 3\n",
    "# Calculate target aspect ratio range (adjust as per your requirement)\n",
    "target_aspect_ratio = 0.6\n",
    "aspect_ratio_range = 0.6\n",
    "selected_candidates = []\n",
    "for i, candidate in enumerate(selective_search_results[1]):\n",
    "    x, y, w, h = candidate['rect']\n",
    "    if w != 0 and h !=0:\n",
    "    # Calculate bounding box size and aspect ratio\n",
    "        bounding_box_size = w * h\n",
    "        aspect_ratio = max(w / h, h / w)\n",
    "    \n",
    "    else:\n",
    "        bounding_box_size = 1\n",
    "        aspect_ratio = 100\n",
    "    # Exclude the bounding box of the whole picture\n",
    "    image_size = image.shape[0] * image.shape[1]\n",
    "    box_coverage = bounding_box_size / image_size\n",
    "    if box_coverage < 0.98 and bounding_box_size >= minimum_bounding_box_size and aspect_ratio <= maximum_aspect_ratio:\n",
    "        selected_candidates.append({'rect': (x, y, w, h), 'index': i})\n",
    "# Function to calculate the coverage ratio between two bounding boxes\n",
    "def calculate_coverage_ratio(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "    intersection_x = max(x1, x2)\n",
    "    intersection_y = max(y1, y2)\n",
    "    intersection_w = min(x1 + w1, x2 + w2) - intersection_x\n",
    "    intersection_h = min(y1 + h1, y2 + h2) - intersection_y\n",
    "    intersection_area = max(0, intersection_w) * max(0, intersection_h)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    return intersection_area / box1_area, box1_area, box2_area\n",
    "# List to store the filtered bounding boxes\n",
    "filtered_boxes = []\n",
    "# Iterate through the selected bounding boxes\n",
    "for i in range(len(selected_candidates)):\n",
    "    current_box = selected_candidates[i]['rect']\n",
    "    is_contained = False\n",
    "    smol = False\n",
    "    # Check if the coverage ratio of the current box is less than 90% of any other box\n",
    "    for j in range(len(selected_candidates)):\n",
    "        if i != j:\n",
    "            other_box = selected_candidates[j]['rect']\n",
    "            coverage_ratio, current_area, other_area = calculate_coverage_ratio(current_box, other_box)\n",
    "            if coverage_ratio >= 0.5 and current_area < other_area:\n",
    "                # The current box is contained within another box\n",
    "                smol = True\n",
    "                break\n",
    "    # If the current box is not contained, add it to the filtered list\n",
    "    if not smol:\n",
    "        filtered_boxes.append(current_box)\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots()\n",
    "# Plot the image\n",
    "ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "rect = []\n",
    "for i, box in enumerate(filtered_boxes):\n",
    "    x, y, w, h = box\n",
    "    rect.append(patches.Rectangle((x, y), w, h, edgecolor='green', facecolor='none', linewidth=2))\n",
    "    ax.add_patch(rect[i])\n",
    "    aspect_ratio = max(w / h, h / w)\n",
    "    ax.text(x, y, f'Box{i}: {round((aspect_ratio - target_aspect_ratio) * 100, 2)}%', color='green')\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veggideas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
