{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from veggideas.transfer import load_non_trainable_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers, layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from veggideas.load_data import load_train_data, load_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 files belonging to 15 classes.\n",
      "Loading training data\n",
      "Training data successfully loaded ✅\n",
      "Found 3000 files belonging to 15 classes.\n",
      "Loading validation data\n",
      "Validation data successfully loaded ✅\n"
     ]
    }
   ],
   "source": [
    "train_data = load_train_data()\n",
    "val_data = load_val_data()\n",
    "base_model = load_non_trainable_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_val_data = val_data.take(5)\n",
    "small_train_data = train_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation():\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,  # Rotate images by 20 degrees\n",
    "        width_shift_range=0.2,  # Shift images horizontally by 20% of the total width\n",
    "        height_shift_range=0.2,  # Shift images vertically by 20% of the total height\n",
    "        shear_range=0.2,  # Apply shear transformation with a shear intensity of 20%\n",
    "        zoom_range=0.2,  # Apply zoom transformation with a zoom range of 20%\n",
    "        horizontal_flip=True,  # Flip images horizontally\n",
    "        fill_mode='nearest'  # Fill newly created pixels during transformations with the nearest value\n",
    "    )\n",
    "\n",
    "    # Create an empty list to store augmented images and labels\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    # Iterate over each batch in the training dataset\n",
    "    for images, labels in train_data:\n",
    "        # Apply data augmentation to the batch\n",
    "        augmented_batch = datagen.flow(images, labels, batch_size=images.shape[0])\n",
    "        augmented_images.extend(augmented_batch[0][0])\n",
    "        augmented_labels.extend(augmented_batch[0][1])\n",
    "\n",
    "    # Convert the augmented images and labels to TensorFlow tensors\n",
    "    augmented_images = tf.convert_to_tensor(augmented_images)\n",
    "    augmented_labels = tf.convert_to_tensor(augmented_labels)\n",
    "\n",
    "    # Create a new dataset using the augmented images and labels\n",
    "    augmented_train_data = tf.data.Dataset.from_tensor_slices((augmented_images, augmented_labels))\n",
    "\n",
    "    return augmented_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###making the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 20s 4s/step - loss: 2.7298 - accuracy: 0.0375 - val_loss: 2.7388 - val_accuracy: 0.0812\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 21s 5s/step - loss: 2.6902 - accuracy: 0.0750 - val_loss: 2.7147 - val_accuracy: 0.0500\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 22s 5s/step - loss: 2.6805 - accuracy: 0.0812 - val_loss: 2.6762 - val_accuracy: 0.0875\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 22s 5s/step - loss: 2.6570 - accuracy: 0.1063 - val_loss: 2.6626 - val_accuracy: 0.0938\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 21s 5s/step - loss: 2.6353 - accuracy: 0.1000 - val_loss: 2.6446 - val_accuracy: 0.1125\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 22s 5s/step - loss: 2.6127 - accuracy: 0.2062 - val_loss: 2.6504 - val_accuracy: 0.1375\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 21s 5s/step - loss: 2.5975 - accuracy: 0.2250 - val_loss: 2.6515 - val_accuracy: 0.1187\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 22s 5s/step - loss: 2.5707 - accuracy: 0.2688 - val_loss: 2.6372 - val_accuracy: 0.1813\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 22s 5s/step - loss: 2.5652 - accuracy: 0.3688 - val_loss: 2.5955 - val_accuracy: 0.2062\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 22s 5s/step - loss: 2.5259 - accuracy: 0.3500 - val_loss: 2.5849 - val_accuracy: 0.2250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b97b940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_last_layers(base_model):\n",
    "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
    "    resize_and_rescale = tf.keras.Sequential([\n",
    "        layers.Rescaling(1./255)])\n",
    "\n",
    "    base_model = base_model\n",
    "    flattening_layer = layers.Flatten()\n",
    "    dense_layer = layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "    dense_layer_2 = layers.Dense(64, activation='relu')\n",
    "    reg_l2 = regularizers.L2(0.001)\n",
    "    dense_layer_reg = layers.Dense(128, activation='relu', bias_regularizer=reg_l2)\n",
    "    conv2D_256 = layers.Conv2D(512, 3, padding='same', activation='relu')\n",
    "    conv2D_512 = layers.Conv2D(512, 3, padding='same', activation='relu')\n",
    "    maxpool_layer = layers.MaxPool2D(pool_size=(2,2))\n",
    "    prediction_layer = layers.Dense(15, activation='softmax')\n",
    "\n",
    "    model = models.Sequential([\n",
    "        resize_and_rescale,\n",
    "        base_model,\n",
    "        conv2D_256,\n",
    "        maxpool_layer,\n",
    "        conv2D_512,\n",
    "        maxpool_layer,\n",
    "        flattening_layer,\n",
    "        dense_layer,\n",
    "        dense_layer_reg,\n",
    "        dense_layer_2,\n",
    "        prediction_layer\n",
    "        ])\n",
    "\n",
    "\n",
    "    opt = optimizers.Adam(learning_rate=1e-5)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model_transfer = add_last_layers(base_model)\n",
    "\n",
    "\n",
    "model_transfer.fit(small_train_data, batch_size=32, epochs=10, validation_data=small_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 15), dtype=tf.float32, name=None))>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[39m# Resize images to the required input size of the object detection model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m resized_images \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mresize(train_data, (\u001b[39m255\u001b[39;49m, \u001b[39m255\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m \u001b[39m# Convert resized images to numpy arrays\u001b[39;00m\n\u001b[1;32m      6\u001b[0m image_arrays \u001b[39m=\u001b[39m resized_images\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/veggideas/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/veggideas/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 15), dtype=tf.float32, name=None))>) with an unsupported type (<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "# Resize images to the required input size of the object detection model\n",
    "resized_images = tf.image.resize(train_data, (255, 255))\n",
    "\n",
    "# Convert resized images to numpy arrays\n",
    "image_arrays = resized_images.numpy()\n",
    "\n",
    "# Run object detection on the batch of images\n",
    "detections = model_transfer(image_arrays)\n",
    "\n",
    "# Extract bounding boxes from the detections\n",
    "batch_bounding_boxes = []\n",
    "for detection in detections:\n",
    "    num_detections = int(detection['num_detections'])\n",
    "    boxes = detection['detection_boxes'][:num_detections]\n",
    "    batch_bounding_boxes.append(boxes)\n",
    "\n",
    "batch_bounding_boxes = np.array(batch_bounding_boxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veggideas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
